{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUlgLdXkn9qW"
   },
   "source": [
    "# Galaxy Classification using PyTorch\n",
    "## Matthew Bartnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjdPece9oEzu"
   },
   "source": [
    "### Importing Initial Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyW1LsQtnuVV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmZhXdsZoLp_"
   },
   "source": [
    "### Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rc0PwrDnnuVW",
    "outputId": "9da9a351-13b1-4b93-daa9-f1ea7fce9ae5"
   },
   "outputs": [],
   "source": [
    "working_folder = 'images_training_rev1'\n",
    "data_labels = pd.read_csv('labels.csv')\n",
    "display(data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re3_Uz4poPT5"
   },
   "source": [
    "### Creating Dataset for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnALCQhgnuVX"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "\n",
    "class GalaxyDataset(Dataset):\n",
    "  def __init__(self, images_folder, labels_path, img_size, num_answers):\n",
    "    self.img_size = img_size\n",
    "    self.image_labels = pd.read_csv(labels_path)\n",
    "    self.images_folder = images_folder\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_labels)\n",
    "\n",
    "  def transform(self, image):\n",
    "    # transformations to apply to image (tensor) after loading\n",
    "    # convert from integer to float for normalization\n",
    "    image = image.float()\n",
    "\n",
    "    # normalize it \n",
    "    image = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(image)\n",
    "    # finally, resize it\n",
    "    image = transforms.Resize(self.img_size)(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # get galaxy ID of image\n",
    "    galaxy_ID = self.image_labels.iloc[idx, 0]\n",
    "    # print(galaxy_ID)\n",
    "\n",
    "    # get filepath of image and load it\n",
    "    image_filename = str(galaxy_ID) + '.jpg'\n",
    "    image_path = os.path.join(self.images_folder, image_filename)\n",
    "\n",
    "    # print(image_path)\n",
    "\n",
    "    # load image and convert to torch Tensor (array-like)\n",
    "    image = read_image(image_path)\n",
    "\n",
    "    # convert image to torch Tensor, normalize and resize\n",
    "    image = self.transform(image)\n",
    "\n",
    "    # get label for image (probabilities of each of the classes) \n",
    "    class_probs = [self.image_labels.iloc[idx, col_idx] for col_idx in range(1, num_answers + 1)]\n",
    "    label = torch.Tensor(class_probs)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9c9fo1hoYX4"
   },
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROKFJqdxnuVY",
    "outputId": "ac908f28-d539-4272-d9e8-39c889351e1a"
   },
   "outputs": [],
   "source": [
    "# build dataset and split it into train and test sets\n",
    "\n",
    "image_size = 32 # should be a sufficient size for shape classification,\n",
    "num_answers = 37 # (or 37 for full Galaxy Zoo decision tree)\n",
    "\n",
    "# but try messing around with it!\n",
    "galaxy_dataset = GalaxyDataset('images_training_rev1', 'labels.csv', image_size, num_answers)\n",
    "\n",
    "# split full dataset into training set and (unseen) test set\n",
    "train_fraction = 0.9\n",
    "test_fraction = 0.1\n",
    "dataset_size = len(galaxy_dataset)\n",
    "print(dataset_size)\n",
    "\n",
    "num_train = int(train_fraction * dataset_size)\n",
    "num_test = dataset_size - num_train\n",
    "print(num_train, num_test)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(galaxy_dataset, [num_train, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gkvw_I2doayE"
   },
   "source": [
    "### Looking at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n9qwPkonuVY",
    "outputId": "5a69695a-ebc3-41e6-8d44-726f2e4740b6"
   },
   "outputs": [],
   "source": [
    "example_batch_size = 5 # number of images and labels to load at once\n",
    "example_loader = DataLoader(train_dataset, \n",
    "                                     batch_size=example_batch_size, \n",
    "                                     shuffle=True # images are loaded in random order\n",
    "                          )\n",
    "\n",
    "\n",
    "# helper function for plotting a batch of images\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def plot_imgbatch(imgs):\n",
    "    imgs = imgs.cpu()\n",
    "    imgs = imgs.type(torch.IntTensor)\n",
    "    plt.figure(figsize=(15, 3*(imgs.shape[0])))\n",
    "    grid_img = make_grid(imgs, nrow=5)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    \n",
    "for batch_index, (images, labels) in enumerate(example_loader):\n",
    "  print('\\n\\n\\nimage batch {}:'.format(batch_index))\n",
    "  plot_imgbatch(images)\n",
    "  print('labels of images from batch {}:'.format(batch_index))\n",
    "  print(labels)\n",
    "  if batch_index > 2:\n",
    "    # plot just a few examples\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzpORW4Nonrl"
   },
   "source": [
    "### Set-up on GPU (CPU can also work, but will take a lot longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "ZKxEbgoonuVZ",
    "outputId": "ce345d87-6c96-4962-ed0b-d71eba77b019"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoBUY_lnpazQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnCmBvXjosyD"
   },
   "source": [
    "### Setting up Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMaTtc8bnuVY"
   },
   "outputs": [],
   "source": [
    "train_batchsize = 1500 # depends on your computation hardware:\n",
    "#  try about 500 for image size of 32, remember that as image size increases, \n",
    "# batch size should decrease\n",
    "eval_batchsize = 100 # batch size for evaluating on test set: can be small\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                                     batch_size=train_batchsize, \n",
    "                                     shuffle=True\n",
    "                                     # images are loaded in random order\n",
    "                                                )\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                        batch_size=eval_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrC2OQWspe6r"
   },
   "source": [
    "### Neural Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB0TYd0npfBK"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "# instantiate classifier from ResNet-18 neural network models\n",
    "net = resnet18()\n",
    "\n",
    "# modify final layer of classifier to output correct number of answers\n",
    "net.fc = torch.nn.Linear(in_features=512, out_features=num_answers, bias=True)\n",
    "\n",
    "\n",
    "# load neural net to GPU device\n",
    "net = net.to(device)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKUBABejplXp"
   },
   "source": [
    "### Loss Function and Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVuNSMMwplcj"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSavVcl9nuVZ"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m69YiJs1qBX6"
   },
   "source": [
    "### Measuring Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-x7mESCnuVZ"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "saved_model_folder = os.path.join(working_folder, 'saved_models')\n",
    "if not os.path.exists(saved_model_folder):\n",
    "  os.makedirs(saved_model_folder)\n",
    "\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "saved_model_filename = 'trainedmodel_{}.pt'.format(date_time)\n",
    "saved_model_path = os.path.join(saved_model_folder, saved_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEQpXqCpqD1I"
   },
   "source": [
    "### Training Network and Looking at Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsGw5IBenuVa",
    "outputId": "cf2c4dc4-3fae-4f86-a86c-b9b4792088db",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_test_error = float('inf')\n",
    "# make the best test error infinitely high by default\n",
    "# (so that the first model is saved)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\t# set network to training mode, so that its parameters can be changed\n",
    "\tnet.train()\n",
    "\n",
    "\t# print training info\n",
    "\tprint(\"### Epoch {}:\".format(epoch))\n",
    "\n",
    "\t# initialize statistics needed to compute overall error/loss\n",
    "\ttrain_error = 0\n",
    "\ttest_error = 0\n",
    "\n",
    "\t# iterate over the training set once\n",
    "\tfor batch_index, (inputs, targets) in tqdm(enumerate(train_loader), total=len(train_loader.dataset)//train_batchsize):\n",
    "\t\t# load the data onto the computation device.\n",
    "\t\t# inputs are a tensor of shape: \n",
    "\t\t#   (batch size, number of channels, image height, image width).\n",
    "\t\t# targets are a tensor of one-hot-encoded class labels for the inputs, \n",
    "\t\t#   of shape (batch size, number of classes)\n",
    "\t\t# in other words, \n",
    "\t\tinputs = inputs.to(device)\n",
    "\t\ttargets = targets.to(device)\n",
    "\n",
    "\t\t# reset changes (gradients) to parameters\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# get the network's predictions on the training set batch\n",
    "\t\tpredictions = net(inputs)\n",
    "\n",
    "\t\t# evaluate the error, and estimate \n",
    "\t\t#   how much to change the network parameters\n",
    "\t\tloss = criterion(predictions, targets)\n",
    "\t\tloss.backward()\n",
    "\t\ttrain_error += loss\n",
    "\n",
    "\t\t# change parameters\n",
    "\t\toptimizer.step()\n",
    "\t\n",
    "\t# overall results on training set\n",
    "\t# error in predicted probabilities\n",
    "\tavg_loss_train = train_error / (batch_index + 1)\n",
    "\tprint(\"Average Training Error (MSE of class probabilities): %.4f\" %(avg_loss_train))\n",
    "\n",
    "\t\n",
    "\t# get results for this epoch on test set\n",
    "\n",
    "\t# evaluating, not training\n",
    "\tnet.eval()\n",
    " \n",
    "\tfor batch_index, (inputs, targets) in tqdm(enumerate(test_loader), total=len(test_loader.dataset)//eval_batchsize):\n",
    "\t\tinputs = inputs.to(device)\n",
    "\t\ttargets = targets.to(device)\n",
    "\t\t# get the network's predictions on the training set batch\n",
    "\t\tpredictions = net(inputs)\n",
    "\n",
    "\t\t# evaluate the error\n",
    "\t\tloss = criterion(predictions, targets)\n",
    "\t\ttest_error += loss\n",
    "\n",
    "\t# error in predicted probabilities\n",
    "\tavg_loss_test = test_error / (batch_index + 1)\n",
    "\tprint(\"Average Test Error (MSE of class probabilities): %.4f\" %(avg_loss_test))\n",
    " \n",
    "\n",
    "  # save trained network as file (make sure you can find it!)\n",
    "  # if the test error was improved/made lower\n",
    "\tif avg_loss_test < best_test_error:\n",
    "\t\tprint('better test performance, saving model...')\n",
    "\t\ttorch.save(net.state_dict(), saved_model_path)\n",
    "\t\n",
    "\t\tbest_test_error = avg_loss_test\n",
    "\n",
    "\t\tprint(torch.save(net.state_dict(), saved_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oeI1H2NqIaL"
   },
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOSMF-dOnuVa",
    "outputId": "d0c1893a-dbe3-4da2-e96e-12b6fccdccff"
   },
   "outputs": [],
   "source": [
    "saved_model_folder = os.path.join(working_folder, 'saved_models')\n",
    "\n",
    "# choose model filename here (****.pt)\n",
    "model_filename = 'trainedmodel_04_19_2023.pt'\n",
    "\n",
    "saved_model_path = os.path.join(saved_model_folder, model_filename)\n",
    "net.load_state_dict(torch.load(saved_model_path))\n",
    "\n",
    "# test it\n",
    "\n",
    "test_error = 0\n",
    "net.eval()\n",
    " \n",
    "for batch_index, (inputs, targets) in tqdm(enumerate(test_loader), total=len(test_loader.dataset)//eval_batchsize):\n",
    "  inputs = inputs.to(device)\n",
    "  targets = targets.to(device)\n",
    "  # get the network's predictions on the training set batch\n",
    "  predictions = net(inputs)\n",
    "\n",
    "  # evaluate the error\n",
    "  loss = criterion(predictions, targets)\n",
    "  test_error += loss\n",
    "\n",
    "# error in predicted probabilities\n",
    "avg_loss_test = test_error / (batch_index + 1)\n",
    "print(\"\\n\\nAverage Test Error (MSE of class probabilities): %.4f\" %(avg_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihTqp624nuVa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
